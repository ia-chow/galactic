{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f438ac568838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = x_test / 255.\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).\\\n",
    "shuffle(60000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i,:,:,0].numpy().astype(\"uint8\"), cmap='gray')\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.experimental.preprocessing.Rescaling(scale= 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = train_dataset.map(lambda x: normalization_layer(x))\n",
    "image_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(first_image), np.max(first_image)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder = (28, 28, 1)\n",
    "input_decoder = (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_encoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_encoder, name='input_layer')\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides= 1, padding='same', name='conv_1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, 2, padding='same', name='conv_3')(x)\n",
    "    x = layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, 1, padding='same', name='conv_4')(x)\n",
    "    x = layers.BatchNormalization(name='bn_4')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_4')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "    \n",
    "    flatten = layers.Flatten()(x)\n",
    "    bottleneck = layers.Dense(2, name='dense_1')(flatten)\n",
    "    model = tf.keras.Model(inputs, bottleneck, name=\"Encoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder(input_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_decoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_decoder, name='input_layer')\n",
    "    x = layers.Dense(3136, name='dense_1')(inputs)\n",
    "    #x = tf.reshape(x, [-1, 7, 7, 64], name='Reshape_Layer')\n",
    "    x = layers.Reshape((7,7,64), name='Reshape_Layer')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides= 1, padding='same',name='conv_transpose_1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_3')(x)\n",
    "    x = layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "    #x = layers.Dropout(rate = 0.25)(x)\n",
    "    \n",
    "    outputs = layers.Conv2DTranspose(1, 3, 1,padding='same', activation='sigmoid', name='conv_transpose_4')(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = decoder(input_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-26e7677e2d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dec' is not defined"
     ]
    }
   ],
   "source": [
    "dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec.save('ae-dec-fashion.h5')\n",
    "# enc.save('ae-enc-fashion.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_loss(y_true, y_pred):\n",
    "    loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "\n",
    "    with tf.GradientTape() as encoder, tf.GradientTape() as decoder:\n",
    "      \n",
    "        latent = enc(images, training=True)\n",
    "        generated_images = dec(latent, training=True)\n",
    "        loss = ae_loss(images, generated_images)\n",
    "        \n",
    "    gradients_of_enc = encoder.gradient(loss, enc.trainable_variables)\n",
    "    gradients_of_dec = decoder.gradient(loss, dec.trainable_variables)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    loss_ = []\n",
    "    for image_batch in dataset:\n",
    "        i += 1\n",
    "        loss = train_step(image_batch)\n",
    "        #loss_.append(loss)\n",
    "\n",
    "    #print(\"Loss\",np.mean(loss_))    \n",
    "    seed = image_batch[:25]\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images([enc,dec],\n",
    "                              epoch + 1,\n",
    "                              seed)\n",
    "    # Save the model every 15 epochs\n",
    "    #if (epoch + 1) % 15 == 0:\n",
    "    #checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    enc.save_weights('tf_ae/fashion/training_weights/enc_'+ str(epoch)+'.h5')\n",
    "    dec.save_weights('tf_ae/fashion/training_weights/dec_'+ str(epoch)+'.h5')\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images([enc,dec],\n",
    "                            epochs,\n",
    "                            seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    latent = enc(test_input, training=False)\n",
    "    predictions = dec(latent, training=False)\n",
    "    print(predictions.shape)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 255, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('tf_ae/fashion/images/image_at_epoch_{:d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(normalized_ds, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.load_weights('../tf_ae/fashion/training_weights/enc_39.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.load_weights('../tf_ae/fashion/training_weights/dec_39.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "for i in normalized_ds:\n",
    "    embed = encoder_model.predict(i)\n",
    "    if embeddings is None:\n",
    "        embeddings = embed\n",
    "    else:\n",
    "        embeddings = np.concatenate((embeddings, embed))\n",
    "    if embeddings.shape[0] > 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_show = 5000\n",
    "figsize = 10\n",
    "\n",
    "index = np.random.choice(range(len(x_test)), n_to_show)\n",
    "example_images = x_test[index]\n",
    "\n",
    "embeddings = enc.predict(example_images)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0] , embeddings[:, 1], alpha=0.5, s=2)\n",
    "plt.xlabel(\"Dimension-1\", size=20)\n",
    "plt.ylabel(\"Dimension-2\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.title(\"Projection of 2D Latent-Space (Fashion-MNIST)\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = min(embeddings[:, 0])\n",
    "max_x = max(embeddings[:, 0])\n",
    "min_y = min(embeddings[:, 1])\n",
    "max_y = max(embeddings[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'T-shirt/top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    "\n",
    "latent = enc.predict(x_test[:25])\n",
    "reconst = dec.predict(latent)\n",
    "\n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    "#fig.subplots_adjust(wspace=-0.021)\n",
    "\n",
    "for i in range(25):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    plt.text(0.5, -0.15, str(label_dict[y_test[i]]), fontsize=15, ha='center', transform=ax.transAxes)\n",
    "    #plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    plt.imshow(reconst[i, :,:,0]*255, cmap = 'gray')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing Fashion Images with Latent-Vector Sampled Uniformly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = min(embeddings[:, 0])\n",
    "max_x = max(embeddings[:, 0])\n",
    "min_y = min(embeddings[:, 1])\n",
    "max_y = max(embeddings[:, 1])\n",
    "\n",
    "x = np.random.uniform(low=min_x,high=max_x, size = (10,1))\n",
    "y = np.random.uniform(low=min_y,high=max_y, size = (10,1))\n",
    "bottleneck = np.concatenate((x, y), axis=1)\n",
    "reconst = dec.predict(bottleneck)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.15, str(np.round(bottleneck[i],1)), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    ax.imshow(reconst[i, :,:,0]*255, cmap = 'gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
