{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8d33f2-593b-4cda-8118-49a3ebb241f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichow9/.conda/envs/wmpl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-28 08:25:07.296285: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 08:25:07.297778: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-28 08:25:07.303579: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-28 08:25:07.322671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745843107.354299  247019 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745843107.363890  247019 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-28 08:25:07.395605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import umap.umap_ as umap\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fc7f1-d508-4586-9145-b0e010e047c9",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff347434-e118-4c1c-902c-160f1361a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../tsne_umap_tutorials/data/APOGEEDR17_GAIAEDR3_noflagfilter.csv', delimiter=',')\n",
    "\n",
    "# data = pd.read_csv('APOGEEDR17_GAIAEDR3_noflagfilter.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89022175-57f4-422a-a4ca-cab174797a8f",
   "metadata": {},
   "source": [
    "Preprocessing (same step as for VAE...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d196b890-1b38-4724-876a-1020c4b36b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 370060 stars in our initial sample\n"
     ]
    }
   ],
   "source": [
    "# Spatial:\n",
    "ra = data[\"RA\"]   #APOGEE\n",
    "dec = data[\"DEC\"]   #APOGEE\n",
    "\n",
    "# Identification:\n",
    "apogee_ID = data[\"# APOGEE_ID_\"]   #APOGEE\n",
    "gaia_ID = data[\"GAIAEDR3_SOURCE_ID\"]  # Gaia\n",
    "\n",
    "# Kinematic:\n",
    "parallax = data[\"GAIAEDR3_PARALLAX\"]  # Gaia\n",
    "pmra = data[\"GAIAEDR3_PMRA\"]  # Gaia\n",
    "pmra_err = data[\"GAIAEDR3_PMRA_ERROR\"]  # Gaia\n",
    "pmdec = data[\"GAIAEDR3_PMDEC\"]  # Gaia\n",
    "pmdec_err = data[\"GAIAEDR3_PMDEC_ERROR\"]  # Gaia\n",
    "RV = data[\"VHELIO_AVG\"]   #APOGEE\n",
    "RV_err = data[\"VERR\"]   #APOGEE\n",
    "#dist = data[\"dist\"]   #APOGEE no dist in this dataset?\n",
    "#dist_err = data[\"dist_err\"]   #APOGEE\n",
    "jr = data[\"jr\"]   #APOGEE\n",
    "jr_err = data[\"jr_err\"]   #APOGEE\n",
    "jz = data[\"jz\"]   #APOGEE\n",
    "jz_err = data[\"jz_err\"]   #APOGEE\n",
    "#jphi = data[\"jphi\"]   #APOGEE no jphi in this dataset?\n",
    "#jphi_err = data[\"jphi_err\"]    #APOGEE\n",
    "\n",
    "# # Spectral (useful for filtering):\n",
    "TEFF_ERR = data[\"TEFF_ERR\"]   #APOGEE\n",
    "TEFF = data[\"TEFF\"]   #APOGEE\n",
    "LOGG_ERR = data[\"LOGG_ERR\"]   #APOGEE\n",
    "LOGG = data[\"LOGG\"]   #APOGEE\n",
    "SNR = data[\"SNR\"]   #APOGEE\n",
    "ASPCAPFLAG = data[\"ASPCAPFLAG\"]\n",
    "STARFLAG = data[\"STARFLAG\"]\n",
    "\n",
    "# # Chemical abundances from astroNN:\n",
    "FE_H = data['FE_H'] \n",
    "C_FE = data['C_FE']\n",
    "CI_FE = data['CI_FE']\n",
    "N_FE = data['N_FE']\n",
    "O_FE = data['O_FE']\n",
    "MG_FE = data['MG_FE']\n",
    "AL_FE = data['AL_FE']\n",
    "SI_FE = data['SI_FE']\n",
    "P_FE = data['P_FE']\n",
    "S_FE = data['S_FE']\n",
    "K_FE = data['K_FE']\n",
    "CA_FE = data['CA_FE']\n",
    "TI_FE = data['TI_FE']\n",
    "TIII_FE = data['TIII_FE']\n",
    "V_FE = data['V_FE']\n",
    "CR_FE = data['CR_FE']\n",
    "MN_FE = data['MN_FE']\n",
    "CO_FE = data['CO_FE']\n",
    "NI_FE = data['NI_FE']\n",
    "\n",
    "# # Chemical abundance errors from astroNN:\n",
    "FE_H_err = data[\"FE_H_ERR\"] \n",
    "C_FE_err = data['C_FE_ERR']\n",
    "CI_FE_err = data['CI_FE_ERR']\n",
    "N_FE_err = data['N_FE_ERR']\n",
    "O_FE_err = data['O_FE_ERR']\n",
    "MG_FE_err = data['MG_FE_ERR']\n",
    "AL_FE_err = data['AL_FE_ERR']\n",
    "SI_FE_err = data['SI_FE_ERR']\n",
    "P_FE_err = data['P_FE_ERR']\n",
    "S_FE_err = data['S_FE_ERR']\n",
    "K_FE_err = data['K_FE_ERR']\n",
    "CA_FE_err = data['CA_FE_ERR']\n",
    "TI_FE_err = data['TI_FE_ERR']\n",
    "TIII_FE_err = data['TIII_FE_ERR']\n",
    "V_FE_err = data['V_FE_ERR']\n",
    "CR_FE_err = data['CR_FE_ERR']\n",
    "MN_FE_err = data['MN_FE_ERR']\n",
    "CO_FE_err = data['CO_FE_ERR']\n",
    "NI_FE_err = data['NI_FE_ERR']\n",
    "\n",
    "# # Number of stars in the initial sample of APOGEE DR16: \n",
    "print(\"There are {} stars in our initial sample\".format(len(ra)))\n",
    "\n",
    "cols = [ra, dec, apogee_ID, gaia_ID, parallax, pmra, pmra_err, pmdec, pmdec_err, RV, RV_err, #dist, dist_err,\n",
    "         jr, jr_err,# jphi, jphi_err,\n",
    "        jz, jz_err, TEFF, TEFF_ERR, LOGG, LOGG_ERR, SNR, ASPCAPFLAG, STARFLAG, FE_H, FE_H_err, C_FE, \n",
    "         C_FE_err, CI_FE, CI_FE_err, N_FE, N_FE_err, O_FE, O_FE_err, MG_FE, MG_FE_err, AL_FE, AL_FE_err, SI_FE,\n",
    "         SI_FE_err, P_FE, P_FE_err, S_FE, S_FE_err, K_FE, K_FE_err, CA_FE, CA_FE_err, TI_FE, TI_FE_err, TIII_FE,\n",
    "         TIII_FE_err, V_FE, V_FE_err, CR_FE, CR_FE_err, MN_FE, MN_FE_err, CO_FE, CO_FE_err, NI_FE, NI_FE_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68921c2e-73ad-4f40-ae6b-a9bd25718846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 144767 stars in our filtered sample\n"
     ]
    }
   ],
   "source": [
    "aspcapflags_filter = np.array(cols[20])==0\n",
    "starflags_filter = np.array(cols[21])==0\n",
    "\n",
    "filters = aspcapflags_filter*starflags_filter\n",
    "\n",
    "\n",
    "filtered_data = []\n",
    "for c in cols:\n",
    "    a = np.array(c)[filters]\n",
    "    filtered_data.append(a)\n",
    "    \n",
    "print(\"There are {} stars in our filtered sample\".format(len(filtered_data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f497ee39-0f7a-42c8-92d7-a42f503af929",
   "metadata": {},
   "outputs": [],
   "source": [
    "FE_H_filtered, C_FE_filtered, CI_FE_filtered = filtered_data[22], filtered_data[24], filtered_data[26]\n",
    "N_FE_filtered, O_FE_filtered, MG_FE_filtered = filtered_data[28], filtered_data[30], filtered_data[32]\n",
    "AL_FE_filtered, SI_FE_filtered, P_FE_filtered = filtered_data[34], filtered_data[36], filtered_data[38]\n",
    "S_FE_filtered, K_FE_filtered, CA_FE_filtered = filtered_data[40], filtered_data[42], filtered_data[44]\n",
    "TI_FE_filtered, TIII_FE_filtered, V_FE_filtered = filtered_data[46], filtered_data[48], filtered_data[50]\n",
    "CR_FE_filtered, MN_FE_filtered, CO_FE_filtered, NI_FE_filtered = filtered_data[52], filtered_data[54], filtered_data[56], filtered_data[58]\n",
    "RV_filtered  = filtered_data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85532dcb-ffcf-478b-b53c-939fbbc6e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "FE_H_err, C_FE_err, CI_FE_err = filtered_data[23], filtered_data[25], filtered_data[27]\n",
    "N_FE_err, O_FE_err, MG_FE_err = filtered_data[29], filtered_data[31], filtered_data[33]\n",
    "AL_FE_err, SI_FE_err, P_FE_err = filtered_data[35], filtered_data[37], filtered_data[39]\n",
    "S_FE_err, K_FE_err, CA_FE_err = filtered_data[41], filtered_data[43], filtered_data[45]\n",
    "TI_FE_err, TIII_FE_err, V_FE_err = filtered_data[47], filtered_data[49], filtered_data[51]\n",
    "CR_FE_err, MN_FE_err, CO_FE_err, NI_FE_err = filtered_data[53], filtered_data[55], filtered_data[57], filtered_data[59]\n",
    "RV_err  = filtered_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2488c70c-1c9c-480c-b158-b2c2587e8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(0.8 * len(filtered_data[0]))\n",
    "test_size = round(0.1 * len(filtered_data[0]))\n",
    "validation_size = round(0.1 * len(filtered_data[0]))\n",
    "\n",
    "training_labels_raw = np.transpose(np.array([FE_H_filtered[:train_size], C_FE_filtered[:train_size], CI_FE_filtered[:train_size], \n",
    "                                        N_FE_filtered[:train_size], O_FE_filtered[:train_size], MG_FE_filtered[:train_size],\n",
    "                                        AL_FE_filtered[:train_size], SI_FE_filtered[:train_size], P_FE_filtered[:train_size],\n",
    "                                        S_FE_filtered[:train_size], K_FE_filtered[:train_size], CA_FE_filtered[:train_size],\n",
    "                                        TI_FE_filtered[:train_size], TIII_FE_filtered[:train_size], V_FE_filtered[:train_size], \n",
    "                                        CR_FE_filtered[:train_size], MN_FE_filtered[:train_size], CO_FE_filtered[:train_size], NI_FE_filtered[:train_size]]))\n",
    "\n",
    "error_training_labels_raw = np.transpose(np.array([FE_H_err[:train_size], C_FE_err[:train_size], CI_FE_err[:train_size], \n",
    "                                        N_FE_err[:train_size], O_FE_err[:train_size], MG_FE_err[:train_size],\n",
    "                                        AL_FE_err[:train_size], SI_FE_err[:train_size], P_FE_err[:train_size],\n",
    "                                        S_FE_err[:train_size], K_FE_err[:train_size], CA_FE_err[:train_size],\n",
    "                                        TI_FE_err[:train_size], TIII_FE_err[:train_size], V_FE_err[:train_size], \n",
    "                                        CR_FE_err[:train_size], MN_FE_err[:train_size], CO_FE_err[:train_size], NI_FE_err[:train_size]]))\n",
    "\n",
    "\n",
    "test_labels_raw = np.transpose(np.array([FE_H_filtered[train_size: train_size + test_size], C_FE_filtered[train_size: train_size + test_size], \n",
    "                                     CI_FE_filtered[train_size: train_size + test_size], N_FE_filtered[train_size: train_size + test_size],\n",
    "                                     O_FE_filtered[train_size: train_size + test_size], MG_FE_filtered[train_size: train_size + test_size],\n",
    "                                        AL_FE_filtered[train_size: train_size + test_size], SI_FE_filtered[train_size: train_size + test_size], \n",
    "                                     P_FE_filtered[train_size: train_size + test_size], S_FE_filtered[train_size: train_size + test_size], \n",
    "                                     K_FE_filtered[train_size: train_size + test_size], CA_FE_filtered[train_size: train_size + test_size],\n",
    "                                        TI_FE_filtered[train_size: train_size + test_size], TIII_FE_filtered[train_size: train_size + test_size], \n",
    "                                     V_FE_filtered[train_size: train_size + test_size], CR_FE_filtered[train_size: train_size + test_size], \n",
    "                                     MN_FE_filtered[train_size: train_size + test_size], CO_FE_filtered[train_size: train_size + test_size], \n",
    "                                     NI_FE_filtered[train_size: train_size + test_size]]))\n",
    "                                     \n",
    "                                     \n",
    "error_test_labels_raw = np.transpose(np.array([FE_H_err[train_size: train_size + test_size], C_FE_err[train_size: train_size + test_size], CI_FE_err[train_size: train_size + test_size], \n",
    "                                        N_FE_err[train_size: train_size + test_size], O_FE_err[train_size: train_size + test_size], MG_FE_err[train_size: train_size + test_size],\n",
    "                                        AL_FE_err[train_size: train_size + test_size], SI_FE_err[train_size: train_size + test_size], P_FE_err[train_size: train_size + test_size],\n",
    "                                        S_FE_err[train_size: train_size + test_size], K_FE_err[train_size: train_size + test_size], CA_FE_err[train_size: train_size + test_size],\n",
    "                                        TI_FE_err[train_size: train_size + test_size], TIII_FE_err[train_size: train_size + test_size], V_FE_err[train_size: train_size + test_size], \n",
    "                                        CR_FE_err[train_size: train_size + test_size], MN_FE_err[train_size: train_size + test_size], CO_FE_err[train_size: train_size + test_size], NI_FE_err[train_size: train_size + test_size]]))\n",
    "\n",
    "\n",
    "validation_labels_raw = np.transpose(np.array([FE_H_filtered[train_size + test_size: train_size + test_size + validation_size], C_FE_filtered[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                     CI_FE_filtered[train_size + test_size: train_size + test_size + validation_size], N_FE_filtered[train_size + test_size: train_size + test_size + validation_size],\n",
    "                                     O_FE_filtered[train_size + test_size: train_size + test_size + validation_size], MG_FE_filtered[train_size + test_size: train_size + test_size + validation_size],\n",
    "                                        AL_FE_filtered[train_size + test_size: train_size + test_size + validation_size], SI_FE_filtered[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                     P_FE_filtered[train_size + test_size: train_size + test_size + validation_size], S_FE_filtered[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                     K_FE_filtered[train_size + test_size: train_size + test_size + validation_size], CA_FE_filtered[train_size + test_size: train_size + test_size + validation_size],\n",
    "                                        TI_FE_filtered[train_size + test_size: train_size + test_size + validation_size], TIII_FE_filtered[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                     V_FE_filtered[train_size + test_size: train_size + test_size + validation_size], CR_FE_filtered[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                     MN_FE_filtered[train_size + test_size: train_size + test_size + validation_size], CO_FE_filtered[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                     NI_FE_filtered[train_size + test_size: train_size + test_size + validation_size]]))\n",
    "                                     \n",
    "                                     \n",
    "error_validation_labels_raw = np.transpose(np.array([FE_H_err[train_size + test_size: train_size + test_size + validation_size], C_FE_err[train_size + test_size: train_size + test_size + validation_size], CI_FE_err[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                        N_FE_err[train_size + test_size: train_size + test_size + validation_size], O_FE_err[train_size + test_size: train_size + test_size + validation_size], MG_FE_err[train_size + test_size: train_size + test_size + validation_size],\n",
    "                                        AL_FE_err[train_size + test_size: train_size + test_size + validation_size], SI_FE_err[train_size + test_size: train_size + test_size + validation_size], P_FE_err[train_size + test_size: train_size + test_size + validation_size],\n",
    "                                        S_FE_err[train_size + test_size: train_size + test_size + validation_size], K_FE_err[train_size + test_size: train_size + test_size + validation_size], CA_FE_err[train_size + test_size: train_size + test_size + validation_size],\n",
    "                                        TI_FE_err[train_size + test_size: train_size + test_size + validation_size], TIII_FE_err[train_size + test_size: train_size + test_size + validation_size], V_FE_err[train_size + test_size: train_size + test_size + validation_size], \n",
    "                                        CR_FE_err[train_size + test_size: train_size + test_size + validation_size], MN_FE_err[train_size + test_size: train_size + test_size + validation_size], CO_FE_err[train_size + test_size: train_size + test_size + validation_size], NI_FE_err[train_size + test_size: train_size + test_size + validation_size]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ea9826-858b-48b9-a4a6-e7f52967ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_labels_raw = np.c_[training_labels_raw, error_training_labels_raw]\n",
    "full_test_labels_raw = np.c_[test_labels_raw, error_test_labels_raw]\n",
    "full_validation_labels_raw = np.c_[validation_labels_raw, error_validation_labels_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22250cfb-1db7-45a3-9575-b9a6020654a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((115814, 38), (14477, 38), (14476, 38))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_labels_raw.shape, full_test_labels_raw.shape, full_validation_labels_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1145170-4b7e-4a54-8d9e-40975f9c2953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.951497  , -2.1234312 , -2.119044  , -0.18978369, -0.7124527 ,\n",
       "        -0.15558027, -0.47890055, -0.15707995, -0.97891414, -0.6047195 ,\n",
       "        -2.2741165 , -0.40278423, -1.2907453 , -0.6157368 , -1.1658391 ,\n",
       "        -1.391077  , -0.64225805, -3.6896837 , -0.2965148 ,  0.01358107,\n",
       "         0.01762647,  0.02378747,  0.01708818,  0.01984493,  0.02071843,\n",
       "         0.02232191,  0.01871411,  0.05818468,  0.01897749,  0.03516773,\n",
       "         0.02518716,  0.02486172,  0.02255343,  0.0375476 ,  0.02254345,\n",
       "         0.02762588,  0.01909786,  0.02221127]),\n",
       " array([5.7460123e-01, 8.3517190e-01, 5.6674075e-01, 3.6616602e+00,\n",
       "        5.5585840e-01, 5.1025960e-01, 4.8934165e-01, 4.5210746e-01,\n",
       "        7.5247210e+00, 9.0582700e-01, 1.0242985e+00, 2.7282690e-01,\n",
       "        5.9501344e-01, 8.7636330e-01, 8.6697330e-01, 3.3210137e-01,\n",
       "        3.5304812e-01, 5.6587050e-01, 2.1270613e-01, 4.5790780e-01,\n",
       "        1.2761971e+00, 1.6784387e+00, 1.8850280e+01, 5.6260234e-01,\n",
       "        5.5715760e-01, 6.7058295e-01, 5.5592450e-01, 2.2155308e+01,\n",
       "        5.8074850e-01, 1.1802338e+04, 1.0069572e+00, 7.6264750e-01,\n",
       "        8.5684610e-01, 1.6342586e+00, 3.9144385e+00, 5.6938570e-01,\n",
       "        2.5924824e+03, 5.7262796e-01]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(full_training_labels_raw.T, axis=1), np.max(full_training_labels_raw.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9f2ca9-dc2a-4a84-a917-72f9e07cb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_threshold = 3 # remove errors that are larger than this...\n",
    "err_mask_train = np.all(np.abs(full_training_labels_raw) < err_threshold, axis=1)\n",
    "err_mask_test = np.all(np.abs(full_test_labels_raw) < err_threshold, axis=1)\n",
    "err_mask_validation = np.all(np.abs(full_validation_labels_raw) < err_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad923299-df10-48c9-9f87-33ab0272dad7",
   "metadata": {},
   "source": [
    "Remove unreasonably huge error bars/abundance estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e753d78-aae2-4e95-bba7-7b48ccee5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_labels = full_training_labels_raw[err_mask_train]\n",
    "full_test_labels = full_test_labels_raw[err_mask_test]\n",
    "full_validation_labels = full_validation_labels_raw[err_mask_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1d8bbe-b7a4-46c4-b6c4-131d48073483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114885, 38), (14374, 38), (14326, 38))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_training_labels.shape, full_test_labels.shape, full_validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f1fe48-24e8-4233-a95d-95d7d8ec5061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.8320073 , -1.2602895 , -2.119044  , -0.18978369, -0.7124527 ,\n",
       "        -0.1388877 , -0.47890055, -0.13207644, -0.97891414, -0.5242982 ,\n",
       "        -0.7713039 , -0.40278423, -1.2907453 , -0.6157368 , -1.1658391 ,\n",
       "        -1.0536897 , -0.64225805, -1.1330373 , -0.2965148 ,  0.01358107,\n",
       "         0.01762647,  0.02378747,  0.01708818,  0.01984493,  0.02071843,\n",
       "         0.02232191,  0.01871411,  0.05818468,  0.01897749,  0.03516773,\n",
       "         0.02518716,  0.02486172,  0.02255343,  0.0375476 ,  0.02254345,\n",
       "         0.02762588,  0.01909786,  0.02221127]),\n",
       " array([0.57460123, 0.81556314, 0.56674075, 1.5901425 , 0.5558584 ,\n",
       "        0.5102596 , 0.48934165, 0.45210746, 1.0318542 , 0.905827  ,\n",
       "        1.0242985 , 0.262527  , 0.59501344, 0.8763633 , 0.8669733 ,\n",
       "        0.27449507, 0.35304812, 0.5658705 , 0.21270613, 0.35667247,\n",
       "        0.48992783, 1.637775  , 1.0633858 , 0.46304193, 0.43964195,\n",
       "        0.55135024, 0.46292186, 2.998515  , 0.55373484, 2.87389   ,\n",
       "        1.0069572 , 0.7626475 , 0.8568461 , 1.6342586 , 1.6748365 ,\n",
       "        0.55016214, 2.8621614 , 0.57262796]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(full_training_labels.T, axis=1), np.max(full_training_labels.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47073612-08e6-4557-b8ee-8e0ac6a33323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42028703-fd73-4345-a6eb-f6789915cc3e",
   "metadata": {},
   "source": [
    "Manual scaler to standardize (equivalent to StandardScaler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5052ddc-e4cd-4a48-959c-17c87d78ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_pca_array = []\n",
    "# standardization_means = np.zeros(len(np.array(x_df_transpose)))\n",
    "# standardization_sigmas = np.zeros(len(np.array(x_df_transpose)))\n",
    "\n",
    "# for i, param in enumerate(np.array(x_df_transpose)):\n",
    "#     mean, sigma = np.mean(param), np.std(param)\n",
    "#     norm = (param - mean)/sigma\n",
    "#     standardized_pca_array.append(norm)\n",
    "#     standardization_means[i] = mean\n",
    "#     standardization_sigmas[i] = sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ea24f-03f1-4975-8a6e-a47ff7bea306",
   "metadata": {},
   "source": [
    "StandardScaler to standardize (same method used for t-SNE/UMAP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8a6ca97-8239-45d6-b0b9-a6bc7462e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split abundances and errors into separate datasets\n",
    "training_labels_abundances = full_training_labels[:, :19]\n",
    "training_labels_errors = full_training_labels[:, 19:]\n",
    "\n",
    "# split abundances and errors into separate datasets\n",
    "test_labels_abundances = full_test_labels[:, :19]\n",
    "test_labels_errors = full_test_labels[:, 19:]\n",
    "\n",
    "# split abundances and errors into separate datasets\n",
    "validation_labels_abundances = full_validation_labels[:, :19]\n",
    "validation_labels_errors = full_validation_labels[:, 19:]\n",
    "\n",
    "standardized_abundance_training_arr = StandardScaler().fit_transform(training_labels_abundances)\n",
    "standardized_abundance_test_arr = StandardScaler().fit_transform(test_labels_abundances)\n",
    "standardized_abundance_validation_arr = StandardScaler().fit_transform(validation_labels_abundances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c459c0b-a60a-4e31-ab8e-cd6bf2c1d366",
   "metadata": {},
   "source": [
    "Begin UMAP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f2dd9-7304-4a8b-bdce-025ea1bd4472",
   "metadata": {},
   "source": [
    "# UMAP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7ca02-7b6f-4edb-ac37-b8afb84f5ddc",
   "metadata": {},
   "source": [
    "Sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d96fc73c-883b-402a-8409-8759a6ede2c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # n neighbors, minimum distances and mses\n",
    "# # 4 major hyperparameters are neighbors, components, metric, min_dist\n",
    "# n_neighbors_ls = [2, 3, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "# min_dists = [0.0, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.99]\n",
    "# training_mses = np.zeros((len(n_neighbors_ls), len(min_dists)))\n",
    "# test_mses = np.zeros((len(n_neighbors_ls), len(min_dists)))\n",
    "\n",
    "# # sweep\n",
    "# for i, n_neighbors in enumerate(n_neighbors_ls):\n",
    "#     for j, min_dist in enumerate(min_dists):\n",
    "#         print(n_neighbors, min_dist)\n",
    "#         # fit using *training* data\n",
    "#         # all default except neighbors/distances, unique=True and pca embedding since spectral seems to fail for some of them...\n",
    "#         reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=2, metric='euclidean', \n",
    "#                             n_epochs=1000, learning_rate=1.0, init='pca', min_dist=min_dist, \n",
    "#                             verbose=True, n_jobs=multiprocessing.cpu_count() - 1, unique=True, \n",
    "#                             random_state=1234, transform_seed=1234)\n",
    "#         fit_umap = reducer.fit(standardized_abundance_training_arr)\n",
    "#         # embed *train* and *test* data\n",
    "#         embed_train = fit_umap.transform(standardized_abundance_training_arr)\n",
    "#         embed_test = fit_umap.transform(standardized_abundance_test_arr)\n",
    "#         # reconstruct *train* and *test* data and record MSE\n",
    "#         reconstruction_training = fit_umap.inverse_transform(embed_train)\n",
    "#         reconstruction_test = fit_umap.inverse_transform(embed_test)\n",
    "#         # record\n",
    "#         training_mses[i, j] = np.mean((standardized_abundance_training_arr - reconstruction_training) ** 2) * 1000  # MSE multiplied by 1000 to match the VAE ones\n",
    "#         test_mses[i, j] = np.mean((standardized_abundance_test_arr - reconstruction_test) ** 2) * 1000  # MSE multiplied by 1000 to match the VAE ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e230400-bc7d-447d-ab87-5e6fe5453d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('umap_training_mses.npy', training_mses)\n",
    "# np.save('umap_test_mses.npy', test_mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d7465-7e91-4a2c-b63b-5f14c4093c0d",
   "metadata": {},
   "source": [
    "Train a bunch of trials with fixed number seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a975db09-d225-4143-8a7e-09035841f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_umap_mse(epochs, n_neighbors=100, min_dist=0.0, training_arr=standardized_abundance_training_arr, test_arr=standardized_abundance_test_arr, \n",
    "             components=2, metric='euclidean', learning_rate=1.0, initialization='spectral', seed=1234):\n",
    "    \"\"\"\n",
    "    Get the MSE for UMAP given epochs, n_neighbors, min_dist, components, metric, epochs, learning rate, initialization, seed\n",
    "    and a set of training and test data\n",
    "\n",
    "    params is tuple of n_neighbors, min_dist\n",
    "    \"\"\"\n",
    "    # fit using *training* data\n",
    "    n_jobs = multiprocessing.cpu_count() - 1 if seed is None else 1\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=components, metric=metric, \n",
    "                        n_epochs=epochs, learning_rate=learning_rate, init=initialization, min_dist=min_dist, \n",
    "                        verbose=False, n_jobs=n_jobs, unique=True, \n",
    "                        random_state=seed, transform_seed=seed) \n",
    "    fit_umap = reducer.fit(training_arr)\n",
    "    # embed *test* data\n",
    "    embed_train = fit_umap.transform(training_arr)\n",
    "    embed_test = fit_umap.transform(test_arr)\n",
    "    # reconstruct *test* data and record MSE of it\n",
    "    reconstruction_training = fit_umap.inverse_transform(embed_train)\n",
    "    reconstruction_test = fit_umap.inverse_transform(embed_test)\n",
    "    # return MSE\n",
    "    training_mse = np.mean((standardized_abundance_training_arr - reconstruction_training) ** 2) * 1000  # MSE multiplied by 1000 to match the VAE ones\n",
    "    test_mse = np.mean((standardized_abundance_test_arr - reconstruction_test) ** 2) * 1000  # MSE multiplied by 1000 to match the VAE ones\n",
    "    return training_mse, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475920bc-8b0b-4a4a-a2b0-e5236ebdef84",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbacc2-9543-43f6-83b5-d4f90cce5466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████████████████████▉                                                                                                                               | 1008/4900 [34:08:07<177:22:05, 164.06s/it]"
     ]
    }
   ],
   "source": [
    "# epochs\n",
    "epochs = list(range(100, 5000))\n",
    "# multiprocess\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1)\n",
    "# run\n",
    "epochs_mses = list(tqdm(pool.imap(get_umap_mse, epochs), total = len(epochs)))\n",
    "# join and close\n",
    "pool.close()\n",
    "pool.join()\n",
    "# save training and test mses\n",
    "training_epochs_mses = epochs_mses[:, 0]\n",
    "test_epochs_mses = epochs_mses[:, 1]\n",
    "# save\n",
    "np.save('umap_training_epochs_mses.npy', training_epochs_mses)\n",
    "np.save('umap_test_epochs_mses.npy', test_epochs_mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc52502-818d-4f7a-8f31-716ec22bc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = range(100, 5000)  # up to 5000 epochs\n",
    "# n_neighbors=100\n",
    "# min_dist=0.0\n",
    "# training_mses_epochs = np.zeros(len(epochs))\n",
    "# test_mses_epochs = np.zeros(len(epochs))\n",
    "\n",
    "# for i, epoch in tqdm(enumerate(epochs)):\n",
    "#     print(epoch)\n",
    "#     reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=2, metric='euclidean', \n",
    "#                     n_epochs=epoch, learning_rate=1.0, init='spectral', min_dist=min_dist, \n",
    "#                     verbose=True, n_jobs=multiprocessing.cpu_count() - 1, unique=True, \n",
    "#                     random_state=1234, transform_seed=1234)\n",
    "#     # fit with training data\n",
    "#     fit_umap = reducer.fit(standardized_abundance_training_arr)\n",
    "#     # embed *train* and *test* data\n",
    "#     embed_train = fit_umap.transform(standardized_abundance_training_arr)\n",
    "#     embed_test = fit_umap.transform(standardized_abundance_test_arr)\n",
    "#     # reconstruct *train* and *test* data and record MSE\n",
    "#     reconstruction_training = fit_umap.inverse_transform(embed_train)\n",
    "#     reconstruction_test = fit_umap.inverse_transform(embed_test)\n",
    "#     # record\n",
    "#     training_mses_epochs[i] = np.mean((standardized_abundance_training_arr - reconstruction_training) ** 2) * 1000  # MSE multiplied by 1000 to match the VAE ones\n",
    "#     test_mses_epochs[i] = np.mean((standardized_abundance_test_arr - reconstruction_test) ** 2) * 1000  # MSE multiplied by 1000 to match the VAE ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b123e-ab0e-4be1-b32b-342e20a0ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('umap_training_mses_epochs.npy', training_mses_epochs)\n",
    "# np.save('umap_test_mses_epochs.npy', test_mses_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba846933-f1d2-4663-aec0-2ab18a25a198",
   "metadata": {},
   "source": [
    "## Extra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6327e46-c50f-4a25-9ae4-6bad539dd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_mse(params, training_arr=standardized_abundance_training_arr, test_arr=standardized_abundance_test_arr, \n",
    "             components=2, metric='euclidean', epochs=200, learning_rate=1.0, initialization='pca', seed=1234):\n",
    "    \"\"\"\n",
    "    Get the MSE for UMAP given params, components, metric, epochs, learning rate, initialization, seed\n",
    "    and a set of training and test data\n",
    "\n",
    "    params is tuple of n_neighbors, min_dist\n",
    "    \"\"\"\n",
    "    # unpack params\n",
    "    n_neighbors, min_dist = params\n",
    "    # fit using *training* data\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=components, metric=metric, \n",
    "                        n_epochs=epochs, learning_rate=learning_rate, init=initialization, min_dist=min_dist, \n",
    "                        verbose=True, n_jobs=multiprocessing.cpu_count() - 1, unique=True, \n",
    "                        random_state=seed, transform_seed=seed) \n",
    "    fit_umap = reducer.fit(training_arr)\n",
    "    # embed *test* data\n",
    "    embedding = fit_umap.transform(test_arr)\n",
    "    # reconstruct *test* data and record MSE of it\n",
    "    reconstruction = fit_umap.inverse_transform(embedding)\n",
    "    # return MSE\n",
    "    return np.mean((test_arr - reconstruction) ** 2) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525139d-f845-40da-a682-d87a91adbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors, min_dist = 1000, 0.99\n",
    "# # fit using *training* data\n",
    "# reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=2, metric='euclidean', \n",
    "#                     n_epochs=200, learning_rate=1.0, init='pca', min_dist=min_dist, \n",
    "#                     verbose=True, n_jobs=multiprocessing.cpu_count() - 1, unique=True) \n",
    "# fit_umap = reducer.fit(standardized_abundance_training_arr)\n",
    "# # embed *test* data\n",
    "# embed_train = fit_umap.transform(standardized_abundance_training_arr)\n",
    "# embedding = fit_umap.transform(standardized_abundance_test_arr)\n",
    "# # reconstruct *test* data and record MSE of it\n",
    "# reconstruction_train = fit_umap.inverse_transform(embed_train)\n",
    "# reconstruction = fit_umap.inverse_transform(embedding)\n",
    "# # return MSE\n",
    "# np.mean((standardized_abundance_training_arr - reconstruction_train) ** 2) * 1000, np.mean((standardized_abundance_test_arr - reconstruction) ** 2) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccad98-a504-4b26-a91d-cb4a853a99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap_mse((1000, 0.99), epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
